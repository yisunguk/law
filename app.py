# ============================================================
# app.py â€” Single-file Streamlit app (clean, non-duplicated)
# ============================================================
from __future__ import annotations

# -------------------- Path bootstrap (app only) --------------------
import os, sys, html, re
try:
    import streamlit as st  # type: ignore
except Exception:
    st = None  # for safety in non-UI contexts

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
MOD_DIR  = os.path.join(BASE_DIR, "modules")
if BASE_DIR not in sys.path:
    sys.path.insert(0, BASE_DIR)
if os.path.isdir(MOD_DIR) and MOD_DIR not in sys.path:
    sys.path.insert(0, MOD_DIR)

# Optional: reduce inotify issues in hosted environments
try:
    if st:
        st.set_option("server.fileWatcherType", "none")
except Exception:
    pass

# -------------------- Project modules (lazy exports) --------------------
# Expectation: modules/__init__.py exposes lazy getters for AdviceEngine, etc.
try:
    from modules import AdviceEngine  # lazy export
except Exception:
    # Fallback to local (in case modules package not found)
    from advice_engine import AdviceEngine  # type: ignore

# Law deep-link helpers (canonical)
try:
    from modules.linking import make_pretty_article_url, resolve_article_url
except Exception:
    # Minimal fallback
    from urllib.parse import quote as _q
    def make_pretty_article_url(law: str, art: str) -> str:
        return f"https://law.go.kr/ë²•ë ¹/{_q((law or '').strip())}/{_q((art or '').strip())}"
    resolve_article_url = make_pretty_article_url

# Optional helpers (safe wrapper / URL utils) â€” app runs without them too.
try:
    from llm_safety import safe_chat_completion  # type: ignore
except Exception:
    safe_chat_completion = None  # graceful fallback

try:
    from external_content import is_url, extract_first_url, extract_all_urls, fetch_article_text  # type: ignore
except Exception:
    def is_url(s: str) -> bool:
        return bool(re.match(r"^https?://", str(s or "")))
    def extract_first_url(s: str) -> str | None:
        m = re.search(r"(https?://\S+)", str(s or ""))
        return m.group(1) if m else None
    def extract_all_urls(s: str) -> list[str]:
        return re.findall(r"(https?://\S+)", str(s or ""))
    def fetch_article_text(url: str, timeout: float = 8.0) -> str:
        return ""

# -------------------- Secrets (Azure) --------------------
try:
    AZURE = st.secrets.get("azure_openai", {}) if st else {}
except Exception:
    AZURE = {}

# -------------------- Azure OpenAI client factory --------------------
try:
    from openai import AzureOpenAI
except Exception:
    AzureOpenAI = None  # runtime guard

def _make_azure_client():
    if AzureOpenAI is None:
        return None
    api_key  = (AZURE.get("api_key") or os.getenv("AZURE_OPENAI_API_KEY") or "").strip()
    endpoint = (AZURE.get("endpoint") or os.getenv("AZURE_OPENAI_ENDPOINT") or "").strip()
    api_ver  = (AZURE.get("api_version") or os.getenv("AZURE_OPENAI_API_VERSION") or "2024-06-01").strip()
    if not (api_key and endpoint and api_ver):
        return None
    cli = AzureOpenAI(api_key=api_key, azure_endpoint=endpoint, api_version=api_ver)
    # router model hint (optional)
    cli.router_model = (
        AZURE.get("router_deployment")
        or AZURE.get("deployment")
        or os.getenv("AZURE_OPENAI_ROUTER_DEPLOYMENT")
        or os.getenv("AZURE_OPENAI_DEPLOYMENT")
        or ""
    )
    return cli

client = _make_azure_client()

# -------------------- Session init --------------------
if st:
    if "messages" not in st.session_state:
        st.session_state.messages = []  # list[{"role":"user|assistant", "content": str}]

# -------------------- LLM ask function --------------------
def ask_llm(question: str) -> str:
    """
    Compose messages from session, add a concise system prompt, then call LLM.
    Uses AdviceEngine if Azure client is ready; otherwise tries safe_chat_completion;
    finally falls back to a static error message.
    """
    sys_prompt = (
        "ë„ˆëŠ” í•œêµ­ ë²•ë¥  ìƒë‹´ ë„ìš°ë¯¸ë‹¤. ê°€ëŠ¥í•œ ê²½ìš° ê·¼ê±°ê°€ ë˜ëŠ” ë²•ë ¹/ì¡°ë¬¸ ë§í¬ë¥¼ í•¨ê»˜ ì œê³µí•˜ê³ , "
        "ì‚¬ì‹¤ì— ê·¼ê±°í•˜ì—¬ ê°„ê²°í•˜ê²Œ ì„¤ëª…í•˜ë¼."
    )
    # Keep the last 6 turns for brevity
    history = [{"role": m["role"], "content": m["content"]} for m in (st.session_state.messages[-6:] if st else [])]
    messages = [{"role": "system", "content": sys_prompt}, *history, {"role": "user", "content": question}]

    # If user included something like "ê±´ì„¤ì‚°ì—…ê¸°ë³¸ë²• ì œ83ì¡°", attach a pretty deeplink hint.
    law_hint = ""
    try:
        m1 = re.search(r"([ê°€-í£A-Za-z0-9]+ë²•)\s*(ì œ?\s*\d+\s*ì¡°(?:\s*ì˜\s*\d+)?|\d+\s*ì¡°)", question)
        if m1:
            law = m1.group(1).strip()
            art = m1.group(2).replace(" ", "")
            if not art.startswith("ì œ"):
                art = "ì œ" + art
            link = resolve_article_url(law, art)
            law_hint = f"ì°¸ê³  ë²•ë ¹ ë§í¬: {link}"
            messages.append({"role":"system","content":law_hint})
    except Exception:
        pass

    # (A) Preferred path: AdviceEngine with Azure client
    if client is not None:
        model = AZURE.get("deployment") or getattr(client, "router_model", "") or "gpt-4o-mini"
        try:
            engine = AdviceEngine(client=client, model=model, temperature=0.2)
            # use non-stream for simplicity in this single-file sample
            answer = engine.generate(messages, stream=False)
            if law_hint and (answer and "http" not in answer):
                answer += f"\n\n{law_hint}"
            return answer
        except Exception as e:
            return f"LLM í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"

    # (B) Fallback: safe_chat_completion if available
    if safe_chat_completion:
        try:
            model = AZURE.get("deployment") or "gpt-4o-mini"
            res = safe_chat_completion(client=None, messages=messages, model=model,
                                       stream=False, temperature=0.2, max_tokens=900)
            if isinstance(res, dict) and res.get("type") == "ok":
                return (res["resp"].choices[0].message.content or "").strip()
            return (res.get("message") or "ì¼ì‹œì ì¸ ë¬¸ì œë¡œ ì‘ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.").strip()
        except Exception as e:
            return f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}"

    # (C) Last fallback: no LLM available
    return "LLM ì„¤ì •ì´ ì—†ì–´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Streamlit secretsì˜ 'azure_openai' ì„¤ì •ì„ í™•ì¸í•´ ì£¼ì„¸ìš”."

# -------------------- UI --------------------
if st:
    st.title("âš–ï¸ ì¸ê³µì§€ëŠ¥ ë²•ë¥  ìƒë‹´ê°€")
    st.caption("êµ­ê°€ë²•ë ¹ì •ë³´ í•œê¸€ì£¼ì†Œ(ë”¥ë§í¬)ì™€ í•¨ê»˜ ì‚¬ì‹¤ ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€í•©ë‹ˆë‹¤. ëŒ€í™” ê¸°ë¡ì€ ì„œë²„ì— ì €ì¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

    # Sidebar: quick deeplink maker
    with st.sidebar:
        st.subheader("ğŸ”— ë²•ë ¹ ë”¥ë§í¬ ë§Œë“¤ê¸°")
        col1, col2 = st.columns(2)
        with col1:
            law_name = st.text_input("ë²•ë ¹ëª…", value="ê±´ì„¤ì‚°ì—…ê¸°ë³¸ë²•")
        with col2:
            art_label = st.text_input("ì¡°ë¬¸", value="ì œ83ì¡°")
        if st.button("ë§í¬ ìƒì„±"):
            url = resolve_article_url(law_name, art_label)
            st.success(f"[{law_name} {art_label}]({url})")

    # Display history
    for m in st.session_state.messages:
        with st.chat_message(m["role"]):
            st.markdown(m["content"])

    # Input
    user_q = st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ê±´ì„¤ì‚°ì—…ê¸°ë³¸ë²• ì œ83ì¡° ë³¸ë¬¸ ì•Œë ¤ì¤˜)")
    if user_q:
        # add user message to session & render immediately
        st.session_state.messages.append({"role": "user", "content": user_q})
        with st.chat_message("user"):
            st.markdown(user_q)

        # get answer
        with st.chat_message("assistant"):
            ans = ask_llm(user_q)
            st.markdown(ans)
        st.session_state.messages.append({"role": "assistant", "content": ans})
else:
    # Non-Streamlit execution (for safety)
    print("This script is intended to run with Streamlit.")
