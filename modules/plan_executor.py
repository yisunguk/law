# êµì²´ë¸”ë¡: plan_executor.py ë‚´ execute_plan í•¨ìˆ˜ ì „ì²´ êµì²´
from typing import Any, Dict

try:
    from .law_fetch import (
        fetch_article_block_by_mst,
        jo_from_art_label as _jo_from_label,
        find_mst_by_law_name,
    )
except Exception:  # pragma: no cover
    from law_fetch import (
        fetch_article_block_by_mst,
        jo_from_art_label as _jo_from_label,
        find_mst_by_law_name,
    )

# linking ëª¨ë“ˆì€ ì„ íƒì 
try:
    from .linking import make_pretty_article_url  # ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ /ë²•ë ¹/ë²•ë ¹ëª…/ì œNì¡°
except Exception:
    try:
        from linking import make_pretty_article_url
    except Exception:
        make_pretty_article_url = None  # type: ignore

# === [REPLACE] modules/plan_executor.py : _scrape_deeplink í•¨ìˆ˜ êµì²´ ===
def _scrape_deeplink(law_name: str, article_label: str, timeout: float = 6.0) -> tuple[str, str]:
    """
    í•œê¸€ ì¡°ë¬¸ ë”¥ë§í¬ í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜¨ ë’¤,
    law_fetch.extract_article_block()ìœ¼ë¡œ 'í•´ë‹¹ ì¡°ë¬¸'ë§Œ ì˜ë¼ì„œ ë°˜í™˜í•œë‹¤.
    """
    if not (law_name and article_label and make_pretty_article_url):
        return "", ""
    try:
        import requests
        from bs4 import BeautifulSoup
        try:
            # ìŠ¬ë¼ì´ì‹± ìœ í‹¸(ìˆìœ¼ë©´ ì‚¬ìš©)
            from .law_fetch import extract_article_block as _slice_article
        except Exception:
            from law_fetch import extract_article_block as _slice_article  # type: ignore

        url = make_pretty_article_url(law_name, article_label)
        r = requests.get(
            url, timeout=timeout,
            headers={
                "User-Agent": "Mozilla/5.0",
                "Accept": "text/html,application/xhtml+xml",
                "Accept-Language": "ko-KR,ko;q=0.9",
            },
            allow_redirects=True,
        )
        if not (200 <= r.status_code < 400):
            return "", url

        html = r.text or ""
        # ìœ ì§€ë³´ìˆ˜/ì°¨ë‹¨/ì˜¤ë¥˜ í˜ì´ì§€ ë°©ì–´
        head = html[:4000]
        bad = ("ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì¡°ë¬¸", "í•´ë‹¹ í•œê¸€ì£¼ì†Œëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤",
               "í˜ì´ì§€ ì ‘ì†ì— ì‹¤íŒ¨í•˜ì˜€ìŠµë‹ˆë‹¤", "ì ‘ê·¼ì´ ì œí•œë˜ì—ˆìŠµë‹ˆë‹¤")
        if any(b in head for b in bad):
            return "", url

        soup = BeautifulSoup(html, "lxml")
        main = (
            soup.select_one("#contentBody")
            or soup.select_one("#conBody")
            or soup.select_one("#conScroll")
            or soup.select_one(".conScroll")
            or soup.select_one("#content")
            or soup
        )
        full_text = (main.get_text("\n", strip=True) or "").strip()

        # â˜… í•µì‹¬: í˜ì´ì§€ ì „ì²´ê°€ ì•„ë‹ˆë¼ 'ìš”ì²­ ì¡°ë¬¸'ë§Œ ìŠ¬ë¼ì´ìŠ¤
        try:
            piece = _slice_article(full_text, article_label) or ""
        except Exception:
            piece = ""

        if not piece:
            # ìŠ¬ë¼ì´ìŠ¤ ì‹¤íŒ¨ ì‹œ ê°„ì´ ì •ê·œì‹ìœ¼ë¡œë¼ë„ êµ¬ê°„ ì¶”ì¶œ
            import re
            art = (article_label or "").strip()
            num = re.sub(r"\D", "", art)  # 83
            # 'ì œ83ì¡°' ~ ë‹¤ìŒ 'ì œ84ì¡°' ì§ì „ê¹Œì§€
            p = re.compile(rf"(ì œ{num}ì¡°(?:ì˜\d+)?[^\\n]*)(.*?)(?=\\nì œ\d+ì¡°|\\në¶€ì¹™|\\Z)", re.S)
            m = p.search(full_text)
            if m:
                piece = (m.group(0) or "").strip()

        return (piece[:4000] if piece else ""), url
    except Exception:
        return "", ""



def execute_plan(plan: Dict[str, Any]) -> Dict[str, Any]:
    """
    LLM ë¼ìš°í„°ê°€ ë§Œë“  planì„ ì‹¤í–‰í•œë‹¤.
    - DRF(JSONâ†’HTML) ìš°ì„ 
    - ì‹¤íŒ¨/ì°¨ë‹¨ ì‹œ í•œê¸€ì£¼ì†Œ(ë”¥ë§í¬) ìŠ¤í¬ë©ì„ 'MST ìœ ë¬´ì™€ ë¬´ê´€í•˜ê²Œ' ìµœí›„ í´ë°±ìœ¼ë¡œ ìˆ˜í–‰
    """
    action = ((plan or {}).get("action") or "").upper()
    if action != "GET_ARTICLE":
        return {"type": "noop", "action": action or "QUICK",
                "message": "execute_plan: GET_ARTICLE ì™¸ ì•¡ì…˜ì€ ì™¸ë¶€ ê²½ë¡œì—ì„œ ì²˜ë¦¬í•˜ì„¸ìš”."}

    law_name  = (plan.get("law_name") or "").strip()
    art_label = (plan.get("article_label") or "").strip()
    mst       = (plan.get("mst") or "").strip()
    jo        = (plan.get("jo") or "").strip()
    efYd_raw  = (plan.get("efYd") or plan.get("eff_date") or "").strip()
    efYd      = "".join(ch for ch in efYd_raw if ch.isdigit())

    # JO ë³´ê°• ('ì œ83ì¡°' â†’ '008300')
    if (not jo) and art_label:
        try:
            jo = _jo_from_label(art_label) or ""
        except Exception:
            jo = ""

    # MST ë³´ê°•(ê°€ëŠ¥í•˜ë©´): DRF ê²€ìƒ‰ â†’ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì§„í–‰
    if (not mst) and law_name:
        try:
            mst = find_mst_by_law_name(law_name, efYd=efYd) or ""
        except Exception:
            mst = ""

    text, link = "", ""

    # 1) DRF(JSONâ†’HTML) ë³¸ë¬¸ ì‹œë„ (MSTê°€ ìˆìœ¼ë©´)
    if mst:
        t1, l1 = fetch_article_block_by_mst(mst, art_label, prefer="JSON", efYd=efYd)
        if not (t1 and t1.strip()):
            t1b, l1b = fetch_article_block_by_mst(mst, art_label, prefer="HTML", efYd=efYd)
            if t1b and t1b.strip():
                t1, l1 = t1b.strip(), l1b
        text, link = (t1 or "").strip(), (l1 or "")

    # 2) ğŸ”´ ìµœí›„ í´ë°±: DRFê°€ ë¹„ì—ˆê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš°, ì¡°ë¬¸ ë”¥ë§í¬ ìŠ¤í¬ë© (MST ìœ ë¬´ ë¬´ê´€)
    if not (text and text.strip()):
        t2, l2 = _scrape_deeplink(law_name, art_label)
        if t2:
            text, link = t2, l2
        elif not link:  # ë³¸ë¬¸ì€ ëª» ê°€ì ¸ì™€ë„ ë§í¬ëŠ” ì œê³µ
            # ë”¥ë§í¬ê°€ ì‹¤íŒ¨í•˜ë©´ ë²•ë ¹ ë©”ì¸ì´ë¼ë„ ëŒë ¤ì¤Œ
            if make_pretty_article_url:
                try:
                    link = make_pretty_article_url(law_name, art_label)
                except Exception:
                    link = ""

    return {
        "type": "article",
        "law": law_name, "article": art_label,
        "mst": mst, "jo": jo, "efYd": efYd,
        "text": (text or "").strip(), "link": link or "",
    }
