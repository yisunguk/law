# --- modules/plan_executor.py (PATCH) ---
# íŒŒì¼ ìƒë‹¨ ì–´ëŠ ìœ„ì¹˜ë“  í•œ ë²ˆë§Œ ì¶”ê°€(ì´ë¯¸ ìˆë‹¤ë©´ ìƒëµ ê°€ëŠ¥)
import logging, time

logger = logging.getLogger("lawbot.deeplink")
if not logger.handlers:
    # ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„±ê²© ê³ ë ¤: ê¸°ë³¸ ë ˆë²¨ë§Œ ì„¤ì •(ì•±ì—ì„œ ì›í•˜ëŠ” ë ˆë²¨ë¡œ ì˜¬ë¦¬ë©´ ë¨)
    logger.setLevel(logging.INFO)

def _dbg(msg: str):
    """streamlitì´ ìˆìœ¼ë©´ í™”ë©´ì—ë„, ì—†ìœ¼ë©´ ë¡œê±°ì—ë§Œ ë‚¨ê¹ë‹ˆë‹¤."""
    try:
        import streamlit as st  # type: ignore
        # st.echo/st.writeëŠ” ì•± ë ˆì´ì•„ì›ƒì— ë§ì¶° ì‚¬ìš©í•˜ì„¸ìš”.
        st.write("ğŸ” [deeplink]", msg)
    except Exception:
        logger.info(msg)


def _scrape_deeplink(law_name: str, article_label: str, timeout: float = 6.0) -> tuple[str, str]:
    """
    í•œê¸€ ì¡°ë¬¸ ë”¥ë§í¬ í˜ì´ì§€ë¥¼ ìŠ¤í¬ë©í•´ 'ìš”ì²­ ì¡°ë¬¸'ë§Œ ì˜ë¼ë‚¸ë‹¤.
    ë°˜í™˜: (ì¡°ë¬¸í…ìŠ¤íŠ¸, í‘œì‹œë§í¬)
    - ë””ë²„ê·¸ ë¡œê·¸: URL, ìƒíƒœì½”ë“œ, HTML ê¸¸ì´, ì»¨í…Œì´ë„ˆ ê¸¸ì´, ìŠ¬ë¼ì´ìŠ¤ ê¸¸ì´/ì„±ê³µì—¬ë¶€
    """
    # ë°©ì–´ ì½”ë“œ: ì¸ì í™•ì¸
    law_name = (law_name or "").strip()
    article_label = (article_label or "").strip()
    if not (law_name and article_label):
        _dbg(f"[skip] ì…ë ¥ ëˆ„ë½ law='{law_name}', article='{article_label}'")
        return "", ""

    # URL ìƒì„±
    try:
        from .linking import make_pretty_article_url  # type: ignore
    except Exception:
        try:
            from linking import make_pretty_article_url  # type: ignore
        except Exception:
            make_pretty_article_url = None  # type: ignore

    if not make_pretty_article_url:
        _dbg("[error] make_pretty_article_url ë¯¸ê°€ìš©")
        return "", ""

    url = make_pretty_article_url(law_name, article_label)
    _dbg(f"[request] {url} (timeout={timeout}s)")

    # HTTP ìš”ì²­
    t0 = time.time()
    try:
        import requests
        from bs4 import BeautifulSoup

        r = requests.get(
            url,
            timeout=timeout,
            allow_redirects=True,
            headers={
                "User-Agent": "Mozilla/5.0",
                "Accept": "text/html,application/xhtml+xml",
                "Accept-Language": "ko-KR,ko;q=0.9",
            },
        )
        dt = (time.time() - t0) * 1000
        html = r.text or ""
        _dbg(f"[response] status={r.status_code}, bytesâ‰ˆ{len(html)}, elapsed={dt:.1f}ms")

        # ìƒë‹¨ ê²½ê³  ì‹œê·¸ë„ ê²€ì‚¬
        head = html[:4000]
        bad_signs = ("ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ì¡°ë¬¸", "í•´ë‹¹ í•œê¸€ì£¼ì†Œëª…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤", "ì ‘ê·¼ì´ ì œí•œë˜ì—ˆìŠµë‹ˆë‹¤")
        if not (200 <= r.status_code < 400):
            _dbg(f"[error] HTTP status={r.status_code} (ë§í¬ í‘œì‹œë§Œ ë°˜í™˜)")
            return "", url
        if any(sig in head for sig in bad_signs):
            _dbg(f"[warn] í˜ì´ì§€ ê²½ê³  ê°ì§€: {[s for s in bad_signs if s in head]}")
            return "", url

        # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì¶”ì¶œ
        soup = BeautifulSoup(html, "lxml")
        main = (
            soup.select_one("#contentBody")
            or soup.select_one("#conBody")
            or soup.select_one("#conScroll")
            or soup.select_one(".conScroll")
            or soup.select_one("#content")
            or soup
        )
        full_text = (main.get_text("\n", strip=True) or "").strip()
        _dbg(f"[parse] container_text_len={len(full_text)}")

        # ì¡°ë¬¸ ë¸”ë¡ë§Œ ìŠ¬ë¼ì´ìŠ¤
        try:
            # ìš°ì„  ì •ì‹ ìŠ¬ë¼ì´ì„œ ì‚¬ìš©
            from .law_fetch import extract_article_block as _slice_article  # type: ignore
        except Exception:
            try:
                from law_fetch import extract_article_block as _slice_article  # type: ignore
            except Exception:
                _slice_article = None  # type: ignore

        piece = ""
        if _slice_article:
            piece = _slice_article(full_text, article_label) or ""

        # ë³´ì¡° ì •ê·œì‹ ìŠ¬ë¼ì´ìŠ¤
        if not piece:
            import re
            num = re.sub(r"\D", "", article_label)
            m = re.search(rf"(ì œ{num}ì¡°(?:ì˜\d+)?[\s\S]*?)(?=\nì œ\d+ì¡°|\në¶€ì¹™|\Z)", full_text)
            piece = (m.group(1) if m else "").strip()

        _dbg(f"[slice] label='{article_label}', piece_len={len(piece)}"
             + (" âœ…" if piece else " âŒ"))

        return (piece[:4000] if piece else ""), url

    except Exception as e:
        _dbg(f"[exception] {type(e).__name__}: {e}")
        return "", url
